{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment out & execute below if you are on google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get install -y \\\n",
    "#     mecab \\\n",
    "#     libmecab-dev \\\n",
    "#     mecab-ipadic-utf8\n",
    "# !pip3 install natto-py==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"https://www.aozora.gr.jp/\"\n",
    "page = requests.get(base_path + \"index_pages/person1779.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html')\n",
    "texts = []\n",
    "for a in soup.find_all(\"a\"):\n",
    "    href = a.get(\"href\")\n",
    "    if isinstance(href, str) and href.startswith(\"../cards\"):\n",
    "        tmp_path = base_path + \"/\".join(href.split(\"/\")[1:])\n",
    "        tmp_page = requests.get(tmp_path)\n",
    "        tmp_soup = BeautifulSoup(tmp_page.text, \"html.parser\")\n",
    "        for tmp_a in tmp_soup.find_all(\"a\"):\n",
    "            tmp_href = tmp_a.get(\"href\")\n",
    "            if (\n",
    "                isinstance(tmp_href, str) and\n",
    "                tmp_href.startswith(\"./files\") and\n",
    "                tmp_href.endswith(\".html\")\n",
    "            ):\n",
    "                source_path = \"/\".join(tmp_path.split(\"/\")[:-1] + [tmp_href[2:]])\n",
    "                source_page = requests.get(source_path)\n",
    "                source_page.encoding = \"shift-jis\"\n",
    "                source_soup = BeautifulSoup(source_page.text, \"html.parser\")\n",
    "                text = (\n",
    "                    source_soup\n",
    "                    .find(\"div\", {\"class\": \"main_text\"})\n",
    "                    .text\n",
    "                    .strip()\n",
    "                    .replace(\"\\n\", \"\")\n",
    "                    .replace(\"\\r\", \"\")\n",
    "                    .replace(\"\\u3000\", \"\")\n",
    "                )\n",
    "                texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_texts = [ts + \"。\" for t in texts for ts in t.split(\"。\")]\n",
    "df = pd.DataFrame(splitted_texts).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from natto import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = MeCab()\n",
    "\n",
    "class MeCabTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang:str): self.lang = 'ja'\n",
    "    def add_special_cases(self, toks:Collection[str]): pass\n",
    "    def tokenizer(self,raw_sentence): return [node.surface for node in nm.parse(raw_sentence, as_nodes=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(MeCabTokenizer, 'ja')\n",
    "processor = [\n",
    "    TokenizeProcessor(tokenizer=tokenizer),\n",
    "    NumericalizeProcessor(max_vocab=60000,min_freq=2)\n",
    "]\n",
    "\n",
    "data_lm = (\n",
    "    TextList\n",
    "    .from_df(df,Path(\"data\"),cols=[0],processor=processor)\n",
    "    .split_by_rand_pct(0.1)\n",
    "    .label_for_lm()\n",
    "    .databunch(bs=64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(16, 1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(\"二十面相\", n_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(\"edogawa.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}